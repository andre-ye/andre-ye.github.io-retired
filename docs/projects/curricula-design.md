---
layout: default
title: Curricula Design
parent: Projects
nav_order: 51
has_children: true
---

# Speech Model Curricula Design

Designing optimal big data curricula for large speech models
{: .fs-6 .fw-300 }

---

| I worked on this research project at Deepgram, a company building advanced speech-to-text APIs. |

Deepgram uses large datasets to train even larger models. While this allows for powerful end-to-end high-capacity speech transcription systems, it also is computationally expensive and restricts the efficiency of experimental R&D. I find that intentionally restricting the dataset can significantly improve the efficiency of model training between two to five times, and particularly results in substantive absolute improvements to the diminishing-returns regime of large model training.

View a writeup [here](https://andre-ye.github.io/files/deepgram/Curriculum_Learning_Deepgram_Final.pdf){:target="_blank"}.

<iframe src="https://andre-ye.github.io/files/deepgram/Curriculum_Learning_Deepgram_Final.pdf" width="100%" height="400" style="border:1px solid black;"></iframe>
