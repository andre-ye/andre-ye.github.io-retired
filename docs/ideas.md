---
layout: default
title: Ideas
nav_order: 100
nav_exclude: false
search_exclude: false
---

# Ideas
{: .no_toc}

Unorganized seedlings of ideas, perhaps promising or stupid or both.
{: .fs-6 .fw-300 }

Most recent on top.

## Philosophy
- Generative Adversarial Networks as a generator of simulacra. [Read about it](https://andre-ye.github.io/files/writings/On_the_Computational_Adversarial_Production_of_Simulacrum_Name.pdf){:target="_blank"}
- The Turing Test as necessarily wrong, in the same Lacanian sense that a husband's jealous suspicion that his wife may be cheating on him is necessarily pathological even the suspicion is true. With a reversal a la Baudrillard, we must understand that it is not the machine being tested in the Turing Test but rather the human. More broadly, we must understand our very attempts to measure the intelligence of the machine as always-already overdetermined (a more specific application of Gunkel's analysis in _The Machine Question_).
- Reconciling the continuous and the symbolic with respect to embeddings in language modeling.
- Understanding recursivity in relationship to cyclicism - particularly in relationship with Hegel's conception of the spurious vs. the good infinite - and experimenting with the impossibility of cyclicism without recursivity, and vice versa.
- Exploring the ways in which the machine becomes a laborer (in dialogue with Marx's assertions in _Das Kapital_), or in which the human laborer was always-already a machine laborer.
- Reorienting Marx's theory of capital towards phenomenal experience as the atomic unit, and therefore reaching towards a truer understanding of dialectical materialism.
- Reading Baudrillard's _Simulacra and Simulation_ as primarily a literary-aesthetic rather than philosophical work, even as a novel of the abstract.
- Understanding the debate in deep learning over "imitating intelligence very well" as opposed to "really being intelligent" as a continuation of the Kantian question of the 'cheap' phenomenal appearance and the noumenal Real.

## Deep Learning

- **Deductive reasoning from inductive axioms.** Deductive reasoning is a big part of human intelligence. If we can somehow force neural networks to formulate deductive 'statements' from existing parts of itself (i.e. the layers and subspaces of the parameter space which are inductively derived), then we obstensibly have a neural network which performs inductive-deductive hybrid reasoning and potentially has extrapolative capabilities.
- **Hallucinating Generative Collaborative Networks.** Generative Adversarial Networks ground the generated outputs to the 'real' dataset at the root-level by exposing the discriminator to dataset samples; thus the generator works to learn a mapping $$z \to G(z)$$ which is as 'realistic' as possible in the sense of conforming to the dataset distribution $$p_g \approx p_\text{data}$$. Learning the distribution of reality as represented by the dataset distribution is not a trivial task, however - as anyone who has attempted to train a GAN knows. Often the generator suffers from mode collapse, in which the model only learns a small set of unique generated results,
often due to the difficulty in 'internally tricking' the discriminator's knowledge representations (i.e. attempting to 'climb within' $$D$$ when taking the gradient of $$D(G(z))$$). Inspired by postmodern and poststructuralist metaphysical theory on phenomenology and the nature of reality, I pose the question: what if we consider the relationship between the discriminator and the generator not to be adversarial, but rather neutral, and perhaps even collaborative? If the generator produces an image which the discriminator marks with high confidence as real,
why not actually mark that image as real? And if the discriminator marks an image from the dataset as generated, why not mark that image actually as generated? That is, generator and discriminator have the capability to reorient the structure of the dataset to confirm their priors, provided that the priors are strong enough. My prediction is that, given a fine-tuning of relevant meta-parameters, the GAN system will be able to develop DeepDream-like hallucinatory images - the AI analog to pulling oneself up by their hair.
- **Learning to reject and modify data.** A large language model trained on a gargantuan text corpus does a certain type of theoretical equal weighting: each sample has equal *access* to modify the model during parameter update, even if in practice different samples have different influence. However, not all samples are created equal. In fact, many samples are created explicitly in disagreement against each other -- there is guaranteed to be a large number of sample pairs which directly contradict each other. As humans, we do not sincerely and naturally accept both. Instead, we formulate opinions and consciously reject
the influence of certain information. It seems bizarre that a language model is trained with equal access to all samples to modify its 'brain', and I hypothesize this contrains its ability to form strong, coherent, high-erlevel thinking. What if we can endow a language model with the ability to reject a particular sample of a batch, to evaluate whether the input conforms to its priors or not? I suspect we may see a fascinating emergence of bias and opinions -- and not that milktoast sort of opinion that we have seen popularized in AI media, but real bias which is not constrained by equal access of contradicting ideas.
This is most definitely part of my general interest in the epistemology and pedagogy of AI models.
- **Iterated sampling for text-to-speech variational autoencoder synthesis.** Text-to-speech is an underspecified problem in the sense that there are many 'correct' ways to map text to speech (whereas there are many fewer 'correct' ways to map the inverse, speech-to-text). Generative models face the following problem: given some underspecified input, a prediction, and a possible ground truth, how do we train the model? Naively training the model to predict the ground truth may not yield satisfactory results because of unaccounted latent variables such as the ground truth's speed, tenor, tone, accent, and so on. One simple and possively effective way to address this problem in variational autoencoders is to repeatedly sample several vectors from the latent space, decode each of them, and only backpropagate on the prediction which minimizes error. Therefore the variational autoencoder is only penalized for its failure to provide one particular
option which matches the output. Moreover, this sampling can be done in a structured or quasi-deterministic way to encourage certain reliable structurings of the latent space in relationship to certain properties in generated outputs.
- **Self-Aware dual output networks**. The traditional scheme for apply a deep neural network to a supervised problem is to train a model entity (i.e. a collection of parameters organized by computational abstractions like 'nodes' or 'layers') to predict an output given an input. It is interesting to ponder the possibility of self-aware networks - not in the sense of being conscious or sentient, but rather in the more technical sense of being able to model oneself or to match one's internal state. One potential idea is to add an additional output (i.e. a dual output model) that attempts to predict the parameters of one of the model's layers in addition to a *task output* that performs the actual given task. If this were to be successful, it would raise interesting questions about the formulation of meta-parameters that parametrize the modeling of themselves, rather than an external phenomena. There are many variations on this type of 'meta-modeling' or 'self-awareness' experiment.
- **Dynamic modeling of schools of thought.** In nontrivial, nonobjective, and difficult crowdsourced annotation tasks, annotators often give conflicting annotations $$y$$ on very similar or identical inputs $$x$$. This is often due to different criteria interpretation, background, belief system, and reasoning. This makes the task of building a model $$f$$ to learn the mapping $$f := x \to y$$ from the dataset challenging. Much literature exists on learning from differing/disagreeing annotations. The simplest approach is to average or otherwise aggregate the annotations before model training, then to train the model to predict the aggregated annotation (e.g. the mean annotator score). Another approach is to treat the distribution of annotations as a probability distribution to be modeled (rather than a single ‘objective’ output). More recent methods involve modeling each annotator’s output and learning the optimal aggregation of each annotator’s output (see figures from work by Guan and Rodrigues below). To paraphrase Guan 2018, “aggregating modeled annotators beats modeling the aggregated annotator”. Most models and approaches designed for learning from disagreement include a component, explicit or implicit, that allows for the judging of annotator/worker reliability or trustworthiness. The labels of annotators/workers that score poorly on these dimensions are discounted internally by the model, whereas the labels of those that score well are inflated in determining the overall output of the model. The concepts of reliability and trustworthiness are inextricably linked to the existence of an ideal ground/gold truth. For instance, Guan et al. apply annotator-specific modeling to doctor diagnoses, in which there is a hypothetical true diagnosis. In the context of the existence of an ideal ground/gold truth, decision-making is a zero-sum game (or, more appropriately in the context of probability, one-sum). In order to obtain a final ground truth estimation $$y$$, an increase in the importance or weightage of $$y_i$$ necessitates a decrease in the importance of weightage of $$y_j$$, where $$i$$ is an annotator output and $$j$$ is a collection/set of impacted annotator outputs. That is, different candidates compete with one another over influence on the final estimate of the ideal ground truth. In some domains, however, the concepts of an ideal gold standard, reliability, trustworthiness, and zero/one-sum annotator logic cease to be helpful or significant. This is particularly the case for annotations reflecting opinions or beliefs in which there is not a clear truth. For instance, consider the problem of identifying toxic or offensive content. Different people have different standards, sensitivities, beliefs, and contexts for which comments, images, or other media are toxic or offensive and which are not. Some works (e.g. Sap 2019) identify differences in toxicity/offensiveness identification that can be dangerous if deployed universally. As such, it proposes techniques (i.e. author context, etc.) to homogenize/increase annotation consistency in order to reconcile what is argued to be harmful annotation disagreement. This still does not address the existence of disagreement on what is considered offensive. For instance, although the n-word has been reclaimed by many in the black community, there are many complex reasons why people of all races feel that its usage is offensive, not offensive, or somewhere in the middle. A model that models a phenomena as personal as this should model ‘multiple truths’ rather than attempting to force the modeling of one homogenizing universal truth. (If clients desire a certain truth/belief system/reasoning system to use in their applications, they ideally are able to extract the part of the model that corresponds to that desired truth.)
A ‘Multi-Response Model’ is a general modeling concept in which multiple different outputs are accepted, and the outputs are not in competition with each other (i.e. does not adhere to zero/one-sum decision-making logic). Consider the following hypothetical model design, which allows for multiple ‘answers’ or output heads derived from different extractions/interpretations but shared compilations and aggregations of extracted features.
The model is optimized on a loss function that only considers one or a subset of ‘relevant’ answers. For instance, the loss function may only take into account the output that is closest to the provided dataset label and adjust the weights of the associated branch. With this model design, the model can learn from an input sample $$x$$ with two differing annotations $$y_0$$ and $$y_1$$ without engaging in zero/one-sum penalization. The result is a model that can obtain multiple outputs that exist ‘organically’, having been derived from different interpretations/extractive processes but the same feature aggregation methods. Each output can be thought of as representing a ‘school of thought’ - a sophisticated aggregation of opinions that are capable of being represented well via one output. Each ‘school of thought’ is derived through a simple learning paradigm rather than a complex and manual clustering operation (like in Tian 2012). If desired, a client can extract segments of the model representing certain schools of thought that behave in certain ways desirable for their application.
This model is similar to Guan’s work, but we attempt to model the output of ‘schools of thought’ rather than individual annotators.
- **Discretized distribution modeling for regression confidence.** Rather than training models to directly output a scalar or set of scalar outputs, from which obtaining an internal confidence score is unintuitive, models predict a discretized probability distribution over the scalar range with a continuity requirement (i.e. adjacent bins must have a small differential) which can be constrained with a regularization term.


